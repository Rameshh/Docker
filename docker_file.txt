* Virtualization: Run multiple application on same machine. bulky in size, running multiple virtual machine leads to unstable performance, bootup take a long time, VM's would not solve the problems like portability, software updates, continous integration and continous deployment.

* Containerization is a type of virtualization which brings virtualization to the operating system level.
* Reason to use containers.
1. Containers have no guest OS and use the host's operating system. So, they share relevant libraries and resources as and when needed.
2. Processing and execution of applications are very fast since application specific binaries and libraries of containers run on the host kernel.
3. Booting up a container takes only a fraction of a second, and also containers are lightweight and faster than virtual machines.

* Docker is a platform which packages an application and all its dependencies together in the form of containers.
* Dockerfile: A Dockerfile is a text document which contains all the commands that a user can call on the command line to assemble an image. docker build to create an automated build to execute.

* Docker Image: Docker Image can be compared to a template which is used to create Docker Containers.
docker run to run the image and create container.
Docker images are stored in the Docker Registry.

* Docker Container: Docker Container is a running instance of a Docker image as they hold the entire package needed to run the application.

* Docker Compose: Docker compose is a YAML file which contains details about the services, networks, and volumes for setting up the Docker application.

* Docker Swarm: Docker Swarm is a technique to create and maintain a cluster of Docker Engines.

* Docker file containes below commands.
1. FROM: Specifies the image that has to be downloaded
2. MAINTAINER: Metadata of the owner who owns the image
3. RUN: Specifies the commands to be executed
4. ENTRYPOINT: Specifies the command which will be executed first
5. EXPOSE: Specifies the port on which the container is exposed

* Install Docker on Ubuntu
1. Let us update the packages.
   sudo apt-get update
2. Install the recommended packages.
   sudo apt-get install linux-image-extra-$(uname -r) linux-image-extra-virtual
3. Install docker engine.
   sudo apt-get install docker-engine
4. Start the docker service.
   sudo service docker start
5. pull a CentOs image from docker hub and run the CentOs container.
   sudo docker pull centos
   sudo docker run -it centos
   
* Install Docker on CentOS
1. Update the system packages and install the required dependencies.
   sudo yum update
   sudo yum install yum-utils device-mapper-persistent-data lvm2
2. Add the Docker stable repository to your system.
   sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
3. Install the Docker.
   sudo yum install docker-ce
4. Start the Docker daemon and enable it to automatically start at boot time.
   sudo systemctl start docker
   sudo systemctl enable docker
5. To verify the Docker service is running.
   sudo systemctl status docker
6. To check Docker version.
   docker -v
   
* What is Docker -> Docker is a containerization platform that packages your application and all its dependencies together
in the form of a docker container to ensure that your application works seamlessly in any environment.

* What is Container -> Docker Container is a standardized unit which can be created on the fly to deploy a particular
application or environment.

* Comparision between Virtual Machine(VM) and Docker Container(DC).
1. Size -> VM required more size and DC required less size.
2. Starup -> VM takes more time bootup and DC takes less time to bootup.
3. Integration -> VM installtion of tools is hectic task and DC installation of tools is easy.

* Docker's Workflow.
1. Docker Daemon -> A persistent backgorund process that manages Docker images, containers, networks, and storage volumes. The Docker
   daemon constantly listens for Docker API requests and processes them.
2. Docker Engine REST API -> An API is used by applications to interact with the Docker daemon. It can be accessed by an HTTP client.
3. Docker CLI -> A command-line interface client for interacting with the Docker daemon.    

* Docker Architecture.
  Docker Architecture includes a Docker client - used to trigger Docker commands, a Docker Host - running the Docker Daemon and a
  Docker Registry - storing Docker Images. The Docker Daemon running within Docker host is responsible for the images and containers.
 
* Docker Objects.
1. Images -> Images are nothing but a read-only binary template that can build containers.
2. Containers -> Containers are sort of encapsulated environments in which you run applications.
3. Networks -> Which all the isolated container communicate.
@ Mainly five network drivers in docker:
  a. Bridge: It is the default network driver for a container.
  b. Host: This driver removes the network isolation between docker containers and docker host.
  c. Overlay: This network swarm services to communicate with each other.
  d. None: This driver disables all the networking.
  e. macvlan: This driver assigns mac address to containers to make them look like physical devices.
4. Storage -> Store data within the writable layer of a container but it requires a storage driver.
@ persistent storage, Docker offers four options:
  a. Data Volumes: They provide the ability to create persistent storage, with the ability to rename volumes, list volumes, 
     and also list the container that is associated with the volume.
  b. Volume Container: It is an alternative approach wherein a dedicated container hosts a volume and to mount that 
     volume to other containers.
  c. Directory Mounts: Another option is to mount a host's local directory into a container.
  d. Storage Plugins: Storage Plugins provide the ability to connect to external storage platforms.

* Docker Registry.
  Docker Registry is where the Docker Images are stored. The Registry can be either a user's local repository or a public 
  repository like a Docker Hub allowing multiple users to collaborate in building an application. Docker registries are 
  services that provide locations from where you can store and download images.
 
* Docker Commands
1. docker -version
   Currently installed version of docker.
2. docker pull 
   docker pull <image name> -> pull images from the docker repository(hub.docker.com).
3. docker run
   docker run -it -d <image name> -> Create a container from an image.
4. docker ps
   List the running containers.
5. docker ps -a
   All the running and exited containers.
6. docker exec
   docker exec -it <container id> bash -> Access the running container.
7. docker stop
   docker stop <container id> -> Stops a running container.
8. docker kill
   docker kill <container id> -> kills the container by stopping its execution immediately.
9. docker commit
   docker commit <container id> <username/imagename>
   Creates a new image of an edited container on the local system.
10. docker login
    Used to login to the docker hub repository.
11. docker push
    docker push <username/image name> ->Push an image to the docker hub repository.
12. docker images
    Lists all the locally stored docker images.
13. docker rm
    docker rm <container id> -> Delete a stopped container.
14. docker rmi
    docker rmi <image-id> -> Delete an image from local storage.
15. docker build
    docker build <path to docker file> -> Build an image from a specified docker file.

* Docker Swarm For Achieving High Availability
* Docker swarm is a technique to create and maintain a cluster of Docker Engines. The Docker engines can be hosted 
  on different nodes, and these nodes which are in remote locations form a cluster when connected in Swarm mode.
* Why use Docker Swarm ?
  Achieving high availability without any downtime is priority for every service provide out there.
* Benefits Of Docker Swarm
1. Docker Swarm does auto load-balancing for us. Hence, there is no need for DevOps engineer to route processing
   requests to other nodes when one fails. The cluster's manager will automatically perform load balancing for us.
2. Decentralized access means all nodes can be accessed easily from the manager.

* Getting Started With Swarm mode
  Docker Swarm is initiated by the manager, or the instance which starts the Swarm cluster becomes the manager.
  1. To start the Swarm cluster
     docker swarm init --advertise-addr ip-address
  2. Swarm cluster is initiated, a token is generated at the manager's end. This token needs to be used by other
     nodes to join the swarm cluster.
     docker swarm join \ --token SWMTKN-1-0c0.....43-2j33.....neco \ 192.168.1.100:2377
  3. Any node that joins the cluster can be later promoted to a manager. In case you want a docker engine to
     join as a manager, execute the below command at the manager's end.
     docker swarm join-token manager
  4. And at a later point in time, if you want the token for a node to join the cluster, run the below command.
     docker swarm join-token node
  5. Run a docker node list command to check how many nodes have joined the cluster along with their status.
     docker node ls

* Creating A Docker Image For Angular App
FROM node:6
Run mkdir -p /usr/src/app
WORKDIR /usr/src/app
COPY package.json /usr/src/spp
RUN npm cache clean
RUN npm install
COPY . /usr/src/app
EXPOSE 4200
CMD ["npm", "start"]

* To create the Docker image based on this Dockerfile, run the below command:
  docker built -t angular-image .
* Starting The Docker Swarm Service
  docker service create --name "Angular-App-Container" -p 4200:4200 angular-image
* Running the docker service list command by service is containerized
  docker service ls
* To identify on which the node/ manager, the app is hosted
  docker service ps Angular-App-Container
* Light on the details about the active container
  docker ps
* How many nodes are running
  docker node ls
* Container running in a particular host
  docker node ps
* Global MODE is a service of this container running at every single/ manager in the cluster. 
  Remember to stop the current service/ container before to spanning another set of containers.  
  docker service rm Angular-App-Container
* The command to spin the container in Global mode is
  docker service create --name "Angular-App-Container" -p 4200:4200 --mode global angular-image
* docker service ps command
  docker service ps Angular-App-Container
* To have 2 replicas of the services running between the three container
  docker service create --name "Angular-App-Container" -p 4200:4200 --replicas=2 angular-image
* Docker Swarm For High Availability
  1. Manually stopping the container from one of the nodes using this command:
     docker stop Angular-App-Container
  2. Run the above command on the node:Worker-1 where the container is running. From 
     the manager run the command:
     docker service ps Angular-App-Container
* High Availability After Scaling Up The Services
  Be it in replicated mode or global mode, we can scale up the number of services running in our cluster.
  And even after scaling up, we will be able to retain high availability.
  1. Assuming that we have either 2 or 3 replicas in our cluster, let us scale up the services to 5.
     docker service scale Angular-App-Container=5
  2. Running the docker service ps command along with the service name, you can see how the 5 services 
     are load balanced and distributed on the 3 nodes. commands are:
     docker service ls
     docker service ps Angular-App-Container
  3. We can drain the manager from hosting any application. The managers are only for managing 
     other workers.
     docker node update --availability drain manager-1
  4. If the manager is now taking part in the cluster by running the docker node list command
     and docker service ps command:
     docker node ls
     docker service ps Angular-App-Container
